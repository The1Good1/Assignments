{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc42bbd",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. Using a graph to illustrate slope and intercept, define basic linear regression.\n",
    "2. In a graph, explain the terms rise, run, and slope.\n",
    "3. Use a graph to demonstrate slope, linear positive slope, and linear negative slope, as well as the different conditions that contribute to the slope.\n",
    "\n",
    "4. Use a graph to demonstrate curve linear negative slope and curve linear positive slope.\n",
    "\n",
    "5. Use a graph to show the maximum and low points of curves.\n",
    "\n",
    "6. Use the formulas for a and b to explain ordinary least squares.\n",
    "\n",
    "7. Provide a step-by-step explanation of the OLS algorithm.\n",
    "\n",
    "8. What is the regression&#39;s standard error? To represent the same, make a graph.\n",
    "\n",
    "9. Provide an example of multiple linear regression.\n",
    "\n",
    "10. Describe the regression analysis assumptions and the BLUE principle.\n",
    "\n",
    "11. Describe two major issues with regression analysis.\n",
    "\n",
    "12. How can the linear regression model&#39;s accuracy be improved?\n",
    "\n",
    "13. Using an example, describe the polynomial regression model in detail.\n",
    "\n",
    "14. Provide a detailed explanation of logistic regression.\n",
    "\n",
    "15. What are the logistic regression assumptions?\n",
    "\n",
    "16. Go through the details of maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd9366",
   "metadata": {},
   "source": [
    "# Ans 1\n",
    "\n",
    "Basic linear regression is a statistical technique used to model the relationship between two variables, typically denoted as X (independent variable) and Y (dependent variable). The goal is to find the best-fitting line that represents the linear relationship between the variables. The line is defined by its slope and intercept.\n",
    "\n",
    "# Ans 2\n",
    "\n",
    "In a graph, the terms rise and run are used to describe the vertical and horizontal distances between two points, respectively. The slope represents the ratio of the rise to the run and quantifies the steepness or incline of a line. It is calculated as the change in the Y-coordinate divided by the change in the X-coordinate between two points on the line.\n",
    "\n",
    "# Ans 3\n",
    "\n",
    "The graph below demonstrates different types of slopes:\n",
    "\n",
    "Linear positive slope: The line slopes upward from left to right, indicating a positive relationship between the variables. As the X-values increase, the corresponding Y-values also increase.\n",
    "Linear negative slope: The line slopes downward from left to right, indicating a negative relationship between the variables. As the X-values increase, the corresponding Y-values decrease.\n",
    "\n",
    "\n",
    "   ^\n",
    "   |\n",
    "   |     *\n",
    "   |    /\n",
    "   |   /\n",
    "   |  /\n",
    "   | /  \n",
    "   |/\n",
    "   +------------------->\n",
    "\n",
    "# Ans 4\n",
    "\n",
    "The graph below demonstrates curve linear slopes:\n",
    "\n",
    "Curve linear negative slope: The line curves downward, indicating a negative relationship between the variables. As the X-values increase, the corresponding Y-values decrease, but the relationship is not linear.\n",
    "Curve linear positive slope: The line curves upward, indicating a positive relationship between the variables. As the X-values increase, the corresponding Y-values increase, but the relationship is not linear.\n",
    "\n",
    "\n",
    "   ^\n",
    "   |\n",
    "   |   *\n",
    "   |  /\n",
    "   | /\n",
    "   |/\n",
    "   +------------------->\n",
    "\n",
    "\n",
    "# Ans 5\n",
    "\n",
    "The graph below shows the maximum and low points of curves:\n",
    "\n",
    "\n",
    "   ^\n",
    "   |\n",
    "   |    .     .\n",
    "   |   / \\   /\n",
    "   |  /   \\ /\n",
    "   | /     X\n",
    "   |/\n",
    "   +------------------->\n",
    "The highest point (maximum) of the curve is represented by the vertex or peak, denoted by \"X.\" The lowest point (minimum) of the curve is the point on the curve with the lowest Y-coordinate.\n",
    "\n",
    "# Ans 6\n",
    "\n",
    "In linear regression, the formulas for the slope (b) and intercept (a) are derived using the ordinary least squares (OLS) method. The slope is calculated as:\n",
    "\n",
    "b = Cov(X, Y) / Var(X)\n",
    "\n",
    "where Cov(X, Y) is the covariance between X and Y, and Var(X) is the variance of X. The intercept is calculated as:\n",
    "\n",
    "a = mean(Y) - b * mean(X)\n",
    "\n",
    "where mean(X) is the mean of X and mean(Y) is the mean of Y.\n",
    "\n",
    "# Ans 7\n",
    "\n",
    "The OLS algorithm for linear regression can be summarized in the following steps:\n",
    "\n",
    "1. Calculate the means of X and Y.\n",
    "\n",
    "2. Calculate the covariance between X and Y.\n",
    "\n",
    "3. Calculate the variance of X.\n",
    "\n",
    "4. Calculate the slope using the formula b = Cov(X, Y) / Var(X).\n",
    "\n",
    "5. Calculate the intercept using the formula a = mean(Y) - b * mean(X).\n",
    "\n",
    "6. The linear regression model is defined as Y = a + b * X.\n",
    "\n",
    "# Ans 8\n",
    "\n",
    "The regression's standard error, also known as the residual standard error, measures the average distance between the observed Y-values and the predicted Y-values by the regression model. It quantifies the dispersion of the data points around the regression line. A graph representing the standard error would show the vertical distances between the observed Y-values and the predicted Y-values.\n",
    "\n",
    "# Ans 9\n",
    "\n",
    "Example of multiple linear regression:\n",
    "\n",
    "Suppose we want to predict a person's weight (Y) based on their height (X1) and age (X2). Multiple linear regression allows us to model this relationship using multiple independent variables. The regression equation would be:\n",
    "\n",
    "Y = a + b1 * X1 + b2 * X2\n",
    "\n",
    "where Y represents weight, X1 represents height, X2 represents age, a represents the intercept, b1 represents the coefficient for height, and b2 represents the coefficient for age.\n",
    "\n",
    "\n",
    "# Ans 10\n",
    "\n",
    "Regression analysis assumptions:\n",
    "\n",
    "1. Linearity: The relationship between the independent variables and the dependent variable is assumed to be linear.\n",
    "2. Independence: The observations in the dataset are assumed to be independent of each other.\n",
    "3. Homoscedasticity: The variance of the errors (residuals) is assumed to be constant across all levels of the independent variables.\n",
    "4. Normality: The errors (residuals) are assumed to follow a normal distribution with a mean of zero.\n",
    "\n",
    "The BLUE principle stands for Best Linear Unbiased Estimators, which states that in linear regression, the estimators for the regression coefficients (slopes) are unbiased and have the minimum variance among all linear unbiased estimators.\n",
    "\n",
    "# Ans 11\n",
    "\n",
    "Two major issues with regression analysis are:\n",
    "\n",
    "    a. Multicollinearity: It occurs when the independent variables in a regression model are highly correlated with each other. This can make it difficult to interpret the individual effects of the variables and can lead to unstable coefficient estimates.\n",
    "    b. Overfitting: It occurs when the regression model is overly complex and captures noise or random fluctuations in the data instead of the true underlying relationship. Overfitting can result in poor generalization to new data.\n",
    "\n",
    "# Ans 12\n",
    "\n",
    "The accuracy of the linear regression model can be improved by:\n",
    "\n",
    "1. Including relevant independent variables: Including variables that are truly associated with the dependent variable can improve the model's accuracy.\n",
    "2. Handling outliers: Outliers can disproportionately influence the regression line. Identifying and appropriately handling outliers can improve the model's fit.\n",
    "3. Transforming variables: Nonlinear relationships can be captured by transforming variables, such as using logarithmic or polynomial transformations.\n",
    "4. Checking model assumptions: Verifying the assumptions of linear regression, such as linearity, independence, and homoscedasticity, and taking appropriate actions if the assumptions are violated.\n",
    "5. Regularization techniques: Techniques like ridge regression or lasso regression can help improve model stability and prevent overfitting.\n",
    "\n",
    "# Ans 13\n",
    "\n",
    "Polynomial regression is a form of regression analysis where the relationship between the independent variable(s) and the dependent variable is modeled using polynomial terms. It extends linear regression by including polynomial terms of higher degrees. For example, a second-degree polynomial regression equation would have terms like X² and X³ in addition to the linear term X. It allows for capturing nonlinear relationships between the variables.\n",
    "\n",
    "\n",
    "# Ans 14\n",
    "\n",
    "Logistic regression is a statistical model used to predict binary outcomes or probabilities. It is commonly used for classification problems where the dependent variable is categorical. Logistic regression models the relationship between the independent variables and the log-odds of the dependent variable using the logistic function (sigmoid function). The logistic function maps the linear combination of the independent variables to a range between 0 and 1, representing the probability of the event occurring.\n",
    "\n",
    "# Ans 15\n",
    "\n",
    "Logistic regression assumptions include:\n",
    "\n",
    "    a. Binary logistic regression assumes that the dependent variable is binary or dichotomous.\n",
    "    b. Independence of observations assumes that the observations are independent of each other.\n",
    "    c. Linearity of the logit assumes that the relationship between the independent variables and the log-odds of the dependent variable is linear.\n",
    "    d. No multicollinearity assumes that there is no perfect multicollinearity among the independent variables.\n",
    "    e. Large sample size assumption suggests that logistic regression performs better with a larger sample size.\n",
    "\n",
    "# Ans 16\n",
    "\n",
    "Maximum Likelihood Estimation (MLE) is a method used to estimate the parameters of a statistical model. In the context of logistic regression, MLE is used to estimate the coefficients that maximize the likelihood of observing the given data under the assumed logistic regression model. The goal is to find the set of coefficients that maximizes the probability of the observed outcomes. The MLE estimation process iteratively adjusts the coefficients until convergence is achieved, typically using optimization algorithms like gradient descent. The estimated coefficients are the ones that provide the best fit to the data according to the logistic regression model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05aafe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
