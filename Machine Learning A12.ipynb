{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc42bbd",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What is prior probability? Give an example.\n",
    "2. What is posterior probability? Give an example.\n",
    "3. What is likelihood probability? Give an example.\n",
    "\n",
    "4. What is Naïve Bayes classifier? Why is it named so?\n",
    "\n",
    "5. What is optimal Bayes classifier?\n",
    "\n",
    "6. Write any two features of Bayesian learning methods.\n",
    "\n",
    "7. Define the concept of consistent learners.\n",
    "\n",
    "8. Write any two strengths of Bayes classifier.\n",
    "\n",
    "9. Write any two weaknesses of Bayes classifier.\n",
    "\n",
    "10. Explain how Naïve Bayes classifier is used for\n",
    "\n",
    "    1. Text classification\n",
    "\n",
    "    2. Spam filtering\n",
    "\n",
    "    3. Market sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd9366",
   "metadata": {},
   "source": [
    "# Ans 1\n",
    "\n",
    "Prior probability refers to the probability assigned to an event or hypothesis before any evidence or data is taken into account. It represents the initial belief or knowledge about the likelihood of an event occurring.\n",
    "\n",
    "Example: Consider a bag of marbles containing red and blue marbles. If we have no additional information, we might assume that the prior probability of drawing a red marble from the bag is 1/2, assuming an equal number of red and blue marbles.\n",
    "\n",
    "# Ans 2\n",
    "\n",
    "Posterior probability refers to the updated probability of an event or hypothesis after considering the available evidence or data. It is obtained by applying Bayes' theorem, which combines the prior probability with the likelihood of the data given the hypothesis.\n",
    "\n",
    "Example: Continuing with the previous example, if we draw a marble from the bag and it is red, the posterior probability of drawing a red marble will be updated based on this evidence. If we assume that the prior probability was 1/2 and the likelihood of drawing a red marble given that it is red is 1, then the posterior probability will be 1.\n",
    "\n",
    "# Ans 3\n",
    "\n",
    "Likelihood probability refers to the probability of observing the given data or evidence, assuming a particular hypothesis or model. It measures how well the hypothesis explains the observed data.\n",
    "\n",
    "Example: Suppose we have a coin and we want to determine whether it is fair or biased. The likelihood probability would represent how likely we would observe the specific sequence of heads and tails if the coin were fair or biased. For example, if we observed the sequence HHTHHT, the likelihood of this sequence given a fair coin might be 1/2^6, while the likelihood given a biased coin might be higher.\n",
    "\n",
    "# Ans 4\n",
    "\n",
    "Naïve Bayes classifier is a probabilistic machine learning algorithm based on Bayes' theorem. It assumes that the features are conditionally independent given the class label, which is a simplifying assumption known as the \"naïve\" assumption. Despite this assumption, the classifier has been found to perform well in many practical applications.\n",
    "\n",
    "It is named \"naïve\" because it assumes independence between features, even though in reality, features may be correlated. However, the simplicity and efficiency of the Naïve Bayes classifier make it a popular choice for many text classification and spam filtering tasks.\n",
    "\n",
    "# Ans 5\n",
    "\n",
    "Optimal Bayes classifier, also known as the Bayes optimal classifier, is a theoretical concept that represents the best possible classifier that can be achieved given the true underlying probability distribution of the data. It assigns the class label that has the highest posterior probability given the observed data.\n",
    "\n",
    "In practice, the optimal Bayes classifier is often not achievable since the true underlying distribution is usually unknown. However, it serves as a benchmark for evaluating the performance of other classifiers.\n",
    "\n",
    "# Ans 6\n",
    "\n",
    "Two features of Bayesian learning methods are:\n",
    "\n",
    "1. Bayesian learning methods provide a principled way of incorporating prior knowledge or beliefs into the learning process. By specifying prior probabilities, Bayesian methods allow the incorporation of prior information about the problem at hand.\n",
    "\n",
    "2. Bayesian learning methods can handle small sample sizes effectively. They are capable of making reasonable predictions even with limited data, as they take into account both the prior knowledge and the observed data.\n",
    "\n",
    "\n",
    "# Ans 7\n",
    "\n",
    "Consistent learners are machine learning algorithms that converge to the true underlying function or model as the amount of training data increases. In other words, as the sample size approaches infinity, consistent learners will make predictions that are increasingly close to the true underlying distribution.\n",
    "\n",
    "# Ans 8\n",
    "\n",
    "Two strengths of the Bayes classifier are:\n",
    "\n",
    "    1. It provides a probabilistic framework that allows for uncertainty estimation. The Bayes classifier provides not only the predicted class label but also the associated probability or confidence measure. This can be valuable in decision-making scenarios where knowing the uncertainty or confidence level is important.\n",
    "\n",
    "    2. The Bayes classifier is computationally efficient and can handle high-dimensional feature spaces. Due to its assumption of feature independence, the classifier can estimate the parameters of the probability distribution for each class separately. This allows for efficient training and prediction, even when dealing with a large number of features.\n",
    "\n",
    "# Ans 9\n",
    "\n",
    "Two weaknesses of the Bayes classifier are:\n",
    "\n",
    "\n",
    "    1. The naïve assumption of feature independence may not hold in real-world applications. In cases where features are correlated or have complex relationships, the Bayes classifier may not capture these dependencies accurately. This can lead to suboptimal performance when the independence assumption is violated.\n",
    "\n",
    "    2. The Bayes classifier requires the estimation of class-conditional probability distributions, which can be challenging in situations where the data is limited or the underlying distributions are complex. Obtaining accurate probability estimates becomes difficult when there are insufficient data points to accurately model the distribution or when the data exhibits non-trivial dependencies.\n",
    "\n",
    "# Ans 10\n",
    "\n",
    "The Naïve Bayes classifier is used for:\n",
    "\n",
    "1. Text classification: Naïve Bayes is widely used for text classification tasks, such as sentiment analysis, spam filtering, and document categorization. The classifier estimates the conditional probability of a class label given the observed words or features in the text. By assuming independence between words, the classifier can efficiently compute the probabilities and make predictions.\n",
    "\n",
    "2. Spam filtering: Naïve Bayes is particularly effective for spam filtering tasks. The classifier is trained on a labeled dataset of spam and non-spam emails, where the features typically include word frequencies or presence/absence indicators. During classification, the classifier computes the probability of an email being spam or non-spam based on the observed word frequencies or features.\n",
    "\n",
    "3. Market sentiment analysis: Naïve Bayes can be used for analyzing market sentiment in financial trading. By training the classifier on historical data where the features represent textual data related to market news, social media posts, or analyst reports, the classifier can predict the sentiment of new texts. This information can be valuable for making trading decisions based on market sentiment.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f069041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
