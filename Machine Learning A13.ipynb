{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc42bbd",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. Provide an example of the concepts of Prior, Posterior, and Likelihood.\n",
    "2. What role does Bayes&#39; theorem play in the concept learning principle?\n",
    "3. Offer an example of how the Nave Bayes classifier is used in real life.\n",
    "\n",
    "4. Can the Nave Bayes classifier be used on continuous numeric data? If so, how can you go about doing it?\n",
    "\n",
    "5. What are Bayesian Belief Networks, and how do they work? What are their applications? Are they capable of resolving a wide range of issues?\n",
    "\n",
    "6. Passengers are checked in an airport screening system to see if there is an intruder. Let I be the random variable that indicates whether someone is an intruder I = 1) or not I = 0), and A be the variable that indicates alarm I = 0). If an intruder is detected with probability P(A = 1|I = 1) = 0.98 and a non-intruder is detected with probability P(A = 1|I = 0) = 0.001, an alarm will be triggered, implying the error factor. The likelihood of an intruder in the passenger population is P(I = 1) = 0.00001. What are the chances that an alarm would be triggered when an individual is actually an intruder?\n",
    "\n",
    "7. An antibiotic resistance test (random variable T) has 1% false positives (i.e., 1% of those who are not immune to an antibiotic display a positive result in the test) and 5% false negatives (i.e., 1% of those who are not resistant to an antibiotic show a positive result in the test) (i.e. 5 percent of those actually resistant to an antibiotic test negative). Assume that 2% of those who were screened were antibiotic-resistant. Calculate the likelihood that a person who tests positive is actually immune (random variable D).\n",
    "\n",
    "8. In order to prepare for the test, a student knows that there will be one question in the exam that is either form A, B, or C. The chances of getting an A, B, or C on the exam are 30 percent, 20%, and 50 percent, respectively. During the planning, the student solved 9 of 10 type A problems, 2 of 10 type B problems, and 6 of 10 type C problems.\n",
    "\n",
    "    a. What is the likelihood that the student can solve the exam problem?\n",
    "\n",
    "    b. Given the student&#39;s solution, what is the likelihood that the problem was of form A?\n",
    "\n",
    "9. A bank installs a CCTV system to track and photograph incoming customers. Despite the constant influx of customers, we divide the timeline into 5 minute bins. There may be a customer coming into the bank with a 5% chance in each 5-minute time period, or there may be no customer (again, for simplicity, we assume that either there is 1 customer or none, not the case of multiple customers). If there is a client, the CCTV will detect them with a 99 percent probability. If there is no customer, the camera can take a false photograph with a 10% chance of detecting movement from other objects.\n",
    "\n",
    "    a. How many customers come into the bank on a daily basis (10 hours)?\n",
    "\n",
    "    b. On a daily basis, how many fake photographs (photographs taken when there is no customer) and how many missed photographs (photographs taken when there is a customer) are there?\n",
    "\n",
    "    c. Explain likelihood that there is a customer if there is a photograph?\n",
    "\n",
    "10. Create the conditional probability table associated with the node Won Toss in the Bayesian Belief network to represent the conditional independence assumptions of the Nave Bayes classifier for the match winning prediction problem in Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd9366",
   "metadata": {},
   "source": [
    "# Ans 1\n",
    "\n",
    "Example of Prior, Posterior, and Likelihood:\n",
    "\n",
    "Let's consider a medical diagnosis scenario for a specific disease.\n",
    "\n",
    "Prior: The prior probability is the initial belief about the probability of a person having the disease before any diagnostic tests are conducted. For example, if we know that the prevalence of the disease in the general population is 1%, then the prior probability of an individual randomly selected from the population having the disease would be 0.01.\n",
    "\n",
    "Likelihood: The likelihood probability represents the probability of observing a particular set of symptoms or test results given that the person has the disease. It measures how well the observed data supports the hypothesis. For instance, if specific symptoms and test results are indicative of the disease with high accuracy, the likelihood of observing those symptoms and results given the disease would be high.\n",
    "\n",
    "Posterior: The posterior probability is the updated probability of a person having the disease after considering the observed symptoms or test results. It is calculated using Bayes' theorem, which combines the prior probability and the likelihood. The posterior probability provides the updated belief about the likelihood of the disease based on the available evidence.\n",
    "\n",
    "# Ans 2\n",
    "\n",
    "Bayes' theorem plays a crucial role in the concept learning principle by providing a framework to update prior beliefs or probabilities based on new evidence. It allows for the incorporation of prior knowledge or beliefs and the adjustment of probabilities based on observed data. In concept learning, Bayes' theorem enables the calculation of the posterior probability of a hypothesis or concept given the observed examples, which forms the basis for decision-making and classification.\n",
    "\n",
    "# Ans 3\n",
    "\n",
    "Example of using the Naïve Bayes classifier in real life:\n",
    "\n",
    "Spam Filtering: Naïve Bayes is commonly used in email spam filtering systems. The classifier is trained on a dataset of labeled emails, where features such as word frequencies or presence/absence indicators are used. The classifier estimates the probability of an email being spam or not spam based on the observed features. During classification, emails are assigned a spam or non-spam label based on the highest posterior probability computed using Bayes' theorem and the naïve assumption of feature independence.\n",
    "\n",
    "# Ans 4\n",
    "\n",
    "Yes, the Naïve Bayes classifier can be used on continuous numeric data. One common approach is to discretize the continuous variables into discrete bins or intervals. The data can be divided into ranges or bins, and then the classifier can estimate the probabilities of each range or bin given the class labels. Another approach is to assume a specific probability distribution for each continuous variable, such as Gaussian (normal) distribution, and estimate the parameters of the distribution for each class. The classifier can then use these probability distributions to calculate the likelihoods and make predictions.\n",
    "\n",
    "# Ans 5\n",
    "\n",
    "Bayesian Belief Networks (BBNs) are graphical models that represent the dependencies between random variables using a directed acyclic graph. BBNs combine probability theory and graph theory to model and reason about uncertain knowledge. Each node in the graph represents a random variable, and the edges represent probabilistic dependencies.\n",
    "\n",
    "BBNs work by propagating probabilities through the network to compute posterior probabilities given observed evidence. They allow for efficient probabilistic inference and can handle complex dependencies between variables. BBNs have applications in various fields, including decision support systems, medical diagnosis, risk assessment, and predictive modeling. They can handle a wide range of issues involving uncertainty and are capable of representing and resolving complex probabilistic relationships.\n",
    "\n",
    "# Ans 6\n",
    "\n",
    "Two features of Bayesian learning methods are:\n",
    "\n",
    "Bayesian learning methods provide a principled approach to handle uncertainty. They assign probabilities to different hypotheses or models and update these probabilities based on observed data, allowing for uncertainty quantification and decision-making under uncertainty.\n",
    "\n",
    "Bayesian learning methods allow the incorporation of prior knowledge or beliefs. By specifying prior probabilities, Bayesian methods enable the integration of existing knowledge or beliefs into the learning process, which can improve learning efficiency and accuracy, particularly in cases with limited data.\n",
    "\n",
    "# Ans 7\n",
    "\n",
    "Consistent learners, in the context of machine learning, refer to learning algorithms that converge to the true concept or target function given sufficient data. A consistent learner is one that guarantees that as the amount of training data approaches infinity, the learner will converge to the correct hypothesis or model. This property ensures that the learner's predictions become increasingly accurate with more data.\n",
    "\n",
    "# Ans 8\n",
    "\n",
    "Two strengths of the Bayes classifier are:\n",
    "\n",
    "    The Bayes classifier can handle high-dimensional data efficiently. Due to the assumption of feature independence in the naïve version, the classifier can estimate the parameters of the probability distribution for each class separately, even when dealing with a large number of features. This makes it computationally efficient compared to other classification methods.\n",
    "\n",
    "    The Bayes classifier is robust to irrelevant features. Since it assumes feature independence, irrelevant features do not affect the classification decision. This property makes it particularly useful in situations where the presence of irrelevant features could introduce noise or uncertainty in the data.\n",
    "\n",
    "# Ans 9\n",
    "\n",
    "Two weaknesses of the Bayes classifier are:\n",
    "\n",
    "1. The naïve assumption of feature independence may not hold in real-world applications. In cases where features are correlated or have complex relationships, the Bayes classifier may not capture these dependencies accurately. This can lead to suboptimal performance when the independence assumption is violated.\n",
    "\n",
    "2. The Bayes classifier requires the estimation of class-conditional probability distributions, which can be challenging in situations where the data is limited or the underlying distributions are complex. Obtaining accurate probability estimates becomes difficult when there are insufficient data points to accurately model the distribution or when the data exhibits non-trivial dependencies.\n",
    "\n",
    "# Ans 10\n",
    "\n",
    "Explanation of how the Naïve Bayes classifier is used for:\n",
    "\n",
    "1. Text classification: Naïve Bayes is widely used for text classification tasks, such as sentiment analysis, spam filtering, and document categorization. The classifier estimates the conditional probability of a class label given the observed words or features in the text. By assuming independence between words, the classifier can efficiently compute the probabilities and make predictions.\n",
    "\n",
    "2. Spam filtering: Naïve Bayes is particularly effective for spam filtering tasks. The classifier is trained on a labeled dataset of spam and non-spam emails, where the features typically include word frequencies or presence/absence indicators. During classification, the classifier computes the probability of an email being spam or non-spam based on the observed word frequencies or features.\n",
    "\n",
    "3. Market sentiment analysis: Naïve Bayes can be used for analyzing market sentiment in financial trading. By training the classifier on historical data where the features represent textual data related to market news, social media posts, or analyst reports, the classifier can predict the sentiment of new texts. This information can be valuable for making trading decisions based on market sentiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719556c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
